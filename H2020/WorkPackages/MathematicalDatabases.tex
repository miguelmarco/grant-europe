\begin{workpackage}[id=dksbases,wphases=1-48!.5,
  title=Data/Knowledge/Software-Bases,lead=JU,
  ZHRM=1,JURM=36,USHRM=12,UWRM=3]

\begin{wpobjectives}
The objectives of this work package are: to design and implement interfaces that can be used for a wide range of mathematical data and to standardise metadata allowing for interoperability, searching, documentation, versioning and visualisation.
\end{wpobjectives}


\begin{wpdescription}
Mathematics is the only science that has not yet benefitted greatly from the systematic interchange of data. At the same time, mathematics has a richer notion of data than other disciplines.
Indeed, "mathematical data" consists of three kinds of objects:
\begin{itemize}
\item[] [D]: proper (numeric/symbolic) data
\item[] [K]:  the knowledge about the mathematical objects given as statements (definitions, theorems or proofs; either formal or rigorously informal)
\item[] [S] : software that computes (with) the mathematical objects
\end{itemize}

All three kinds of "data" are equally important for mathematics and are tightly interlinked:
\begin{itemize}
\item[] [D] serves as examples for [K] or as counterexamples for conjectures in [K];
\item[] [S] computes [D] and establishes properties of [D] (given as [K]);
\item[] [D] tests [S], [S] is verified with respect to [K];
\item[] theorems and proofs in [K] induce and justify algorithms for [S];
\item[] [D] induces conjectures and guides proofs in [K].
\end{itemize}

Many mathematical databases now exist, but their internal structure does not reveal this richness. This weakness prevents the formulation of new conjectures, the testing of new hypotheses, and generally an exploratory approach to mathematical data. The past has shown that such an approach can be fruitful: 
\begin{itemize}
\item both the Riemann Hypothesis and the Birch and Swinnerton-Dyer conjectures resulted from exploratory $L$-function computations, and now stand among the seven Clay Millenium Problems;
\item the Monstrous Moonshine conjecture finds its origin in a numerical co\"incidence between dimensions of representations of the Monster group and coefficients of the $j$-function, and its conclusion eventually led to Borcherds' Fields medal.
\end{itemize}

Therefore to facilitate future advances, we need ways to represent DKS in the same systems, and -- since current computational/experimental mathematics involve extensive DKS -- we need a new kind of "database", which we will call Mathematical Data/Knowledge/Software-bases.

This complexity is on vivid display in the \emph{L-functions and Modular Forms database} project (\LMFDB): while the general shape of the functional equation of an $L$-function is dependent on a lot of theoretical knowledge, it also requires parameter data and the coefficients of the associated Dirichlet series. Once this is obtained, highly optimised (and heavily parallelizable) algorithms can be run to compute values of this function. 

We propose in this work package to design and build an infrastructure that would make it
easy for either individual mathematicians or a distributed collaboration to manage and use
such interlinked mathematical data. This work would provide part of the backend to Work
Packages \TODO{work package on interfaces, and???}, and would draw on previous work with
the \LMFDB and FindStat (which will be treated as prototypes for our purposes, to serve as
exemplars to other projects) and in return will substantially enhance their
capabilities. Prerequisites should be kept to a minimum (depending on contributors' and
users' needs and goals), and in particular would not require any background in databases
to contribute new data or perform queries.
\end{wpdescription}
\begin{tasklist}
\begin{task}[title=Survey of existing databases,id=data-assessment]
All the systems considered in this proposal (\GAP, \Sage, \Pari, \Singular) include data as part of their regular distribution. In this task, we will survey existing databases, the technology used to implement them, how they were linked to the rest of the existing infrastructure and the functionalities offered. We will also select additional external data and projects to add to this effort, aiming to maximise the impact of our work. 
\end{task}

\begin{task}[title={Design of new infrastructure, formulation of requirements}, id=data-design]
Ontologies are the canonical method used to implement databases that require significant data interchange. However, because of extreme reification in mathematics, this is not entirely suitable for our goals. We will design a new infrastructure for \TheProject, drawing on existing emerging standards. 

We will organise a workshop associated to this task.
\end{task}

\begin{task}[title=Triform Theories in OMDoc/MMT,id=data-triform]
OMDoc/MMT is a representation language for mathematical knowledge and documents. Carette and Farmer have developed the notion of biform theories (K/S) in a uniform representational approach; our work here would extend this along the data axis, which will require a specialised but integrated treatment.
\end{task}

\begin{task}[title=Computational Foundation for Python/Sage (or some CAS),id=data-foundationCAS]
In the OMDoc/MMT world a foundation is a logical base language that gives the formal meaning to all objects represented/formalized in it. We have created a very initial computational foundation for Scala and implemented it in the MMT API. This can be used to execute (or verify) computations directly in OMDoc/MMT and thus forms the basis for various integration tasks for OMDoc/MMT biform theories that integrate Scala computations. Here we propose to develop a somewhat more complete computational foundation for Python and/or parts of Sage (coverage to be determined). Bi/Triform theories come in three parts:
\begin{itemize}
\item syntax: what operators/types are there, how do they nest, 
\item computation:  what does the computation relation look like (sometimes called operational semantics). The declarative semantics of a computational foundation can be given as an OMDoc/MMT theory morphism into another foundation (e.g. a set theory);
\item ??? three parts
\end{itemize}
\end{task}

\begin{task}[title=OEIS Case Study (Coverage and automated import),id=data-OEIS]
  In this case study we test the practical coverage of the trifunctional modules, by
  transforming an existing, high-profile database (the Online Sequence of Integer
  Sequences http://www.oeis.org) into OMDoc/MMT. The OEIS has about 250 thousand
  sequences, with formulae, descriptions, definitions, references, software, etc. in a
  structured text file (but no standardized format for formulae and references), so we
  expect to get 250 k theories. Having the OEIS in OMDoc/MMT form allows to do Knowledge
  Management services (presentation, definition lookup, formula search, ...) in
  MathHub.info (see WP4.?). The OEIS is a good case study, since the DKM are licensed
  under a CC license which allows derived works. The large size will allow statistically
  significant semantic cross-validation of the heuristic transformation process and thus
  achieve a significant DKS community resource.
\end{task}

% Michael, I think triformal theories would be easier to start with findstat.org
% There are many reasons: more consistent structure in the mathematical data, more established research patterns, more consistent database storage, tighter integration of the code with sage code (in fact copy paste), etc

\begin{task}[title=FindStat Case Study (triformal theories),id=data-findstat]
  In this task we would develop triformal theories for the FindStat project to test the
  design from \localtaskref{data-foundationCAS}.  Similarly to the previous task, in this
  case study, we first develop a thorough OMDoc/MMT model, which should only involve a
  handful of MMT theories (combinatorial collections, maps, statistics,...), each with a
  few hundred realisations. Together with \TOWRITE{POD}{WP4}, this will again allow for
  easier knowledge management services, and in particular improved search services.

  This Task will be co-developed with \localtaskref{data-foundationCAS}, it will validate
  the design of triformal theories and be iterated to test the design changes.
\end{task}

\begin{task}[title=\LMFDB Case study (triformal theories),id=data-LMFDB]
  In this task we would develop triformal theories for an exemplary part of the \LMFDB
  project to test the design from \localtaskref{data-foundationCAS}.  We will identify a
  fragment of the \LMFDB that we want to model and design the model. Then we will perform
  cross-validation of the three model parts against each other (essentially model-based
  testing of software and inference). Finally, we will pick an algorithm from the LFMDB
  and verify it against its specification and the computational foundation developed in
  \localtaskref{data-foundationCAS}. (decrease importance of verification as opposed to
  interoperability)
\end{task}



\begin{task}[title=Memoization and production of new data,id=data-memo]
Many CAS users run large and intensive computations, for which they want to collect the results while simultaneously working on software improvements. \Sage currently has a limited \lstinline{cached_method}, that is not persistent across sessions and does not enable to publish the result or share it with a smaller group of collaborators. We propose to use, extend and contribute back to some established persistent memoization infrastructure, such as \texttt{python-joblib}, \texttt{redis-simple-cache} or \texttt{dogpile.cache}. The caching should apply recursively to lower level functions, and should be trivial to setup and configure for the end user: in a single line, the user only needs to select an existing function and maybe provide some additional semantic information, and has the option to change the defaults for a few parameters, such as the backend (shared dropbox folder, remote directory, database, git repository, ...). The interface could be through a \Python decorator. 
Additionally, it should be easy to launch a data-bot to populate the database, all the versioning and provenance tracking should be handled (user, algorithm, software version, ...), and the system should have useful data properties (atomicity, merging, and error detection). 
%Mock code:
%    \begin{lstlisting}
%       mycloud = storage("ssh:xxx@yyy/zzz")
%       memoize(sage.combinat...., storage=mycloud, input=ZZ, output=Posets(), key="catalan")
%    \end{lstlisting}
\end{task}
\end{tasklist}

\begin{wpdelivs}
  \begin{wpdeliv}[due=12,id=conv,dissem=PU,nature=DEC]
        {Conversion of existing and new databases to unified interoperable system}
     \begin{itemize}
     \item Polytopes in Polymake
     \item graphs, graph properties
     \item Finite groups (Max)
     \item Lattices
     \end{itemize}
   \end{wpdeliv}
  \begin{wpdeliv}[due=24,id=persistent-memoization,dissem=PU,nature=O]
    {Shared persistent memoization library for Python/Sage} 
    Recomputation?,  Ease of publishing, importing, ...
  \end{wpdeliv}
  \begin{wpdeliv}[due=9,id=wsrep,dissem=PU,nature=R]{Workshop Report}
  \end{wpdeliv}
  \begin{wpdeliv}[due=12,id=dkstheories,dissem=PU,nature=R]
        {Design of Triform (DKS) Theories (Specification/RNC Schema/Examples)}
  \end{wpdeliv}
  \begin{wpdeliv}[due=24,id=dksimp,dissem=PU,nature=O]
        {Implementation of Triform Theories in the MMT API.}
  \end{wpdeliv}
  \begin{wpdeliv}[due=12,id=pssyntax,dissem=PU,nature=DEC]
        {Python/Sage Syntax Foundation Module in OMDoc/MMT}
  \end{wpdeliv}
  \begin{wpdeliv}[due=24,id=psfoundation,dissem=PU,nature=O]
        {Python/Sage Computational Foundation Module in OMDoc/MMT}
  \end{wpdeliv}
  \begin{wpdeliv}[due=36,id=pssem,dissem=PU,nature=O]
      {Python/Sage Declarative Semantics in OMDoc/MMT}
  \end{wpdeliv}
  \begin{wpdeliv}[due=12,id=lfmmod,dissem=PU,nature=R]
      {\LMFDB deep modelling: Fragment Identification \& Initial Model Design}
  \end{wpdeliv}
  \begin{wpdeliv}[due=18,id=lfmval,dissem=PU,nature=R]
      {\LMFDB Data vs. Knowledge vs. Software Validation}
  \end{wpdeliv}
  \begin{wpdeliv}[due=36,id=lfmverif,dissem=PU,nature=O]
      {\LMFDB Algorithm verification wrt. a Triformal theory}
  \end{wpdeliv}
  \begin{wpdeliv}[due=46,id=lfmint,dissem=PU,nature=R]
      {\LMFDB full integration of algorithms, data and presentation (not so much verification)}
  \end{wpdeliv}
  \begin{wpdeliv}[due=9,id=oeisparser,dissem=PU,nature=O]
      {Heuristic Parser for the OEIS}
  \end{wpdeliv}
  \begin{wpdeliv}[due=18,id=oeisvalidation,dissem=PU,nature=R]
      {Cross-Validation for OEIS DKS-Theories}
  \end{wpdeliv}
\end{wpdelivs}


Another connection: on several occasions, we found that software was the best way to
represent certain databases of mathematical knowledge. E.g. in Algebraic Combinatorics we
have a whole zoo of Hopf algebras. Many of them are implemented in MuPAD/Sage by
specifying the objects that index the basis together with computation rules for the
product and coproduct. When we want to retrieve information about such algebras, it's
usually much more convenient to look at the code than to search through the
literature. Especially since the code is usually more correct than the literature because
it's *tested*.


We may also think of providing an interface to \LMFDB via SCSCP
protocol (http://www.symbolic-computing.org/scscp) so it may
be accessed by a variety of other systems (see their current
list at http://www.symbolic-computing.org/scscp)


database access to \LMFDB as a python library
\end{workpackage}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../proposal"
%%% End:

%  LocalWords:  workpackage dksbases wphases wpobjectives standardise visualisation emph
%  LocalWords:  wpdescription Swinnerton-Dyer Millenium Borcherds optimised tasklist conv
%  LocalWords:  parallelizable maximise organise biform specialised trifunctional TOWRITE
%  LocalWords:  triformal findstat.org data-findstat localtaskref realisations texttt wrt
%  LocalWords:  Memoization python-joblib texttt redis-simple-cache texttt dogpile.cache
%  LocalWords:  lstlisting mycloud memoize sage.combinat wpdelivs wpdeliv dissem Polymake
%  LocalWords:  Recomputation wsrep dkstheories dksimp pssyntax psfoundation pssem lfmmod
%  LocalWords:  modelling lfmval lfmverif lfmint oeisparser oeisvalidation Hopf coproduct
